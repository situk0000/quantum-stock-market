# -*- coding: utf-8 -*-
"""Quantum analysis.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Ck3-WgyxU7dZZC_GP0ZSmVDRt4aYuU0n
"""

# Commented out IPython magic to ensure Python compatibility.
# %pip install pennylane

import pennylane as qml
from pennylane import numpy as np
import pandas as pd
import joblib
from sklearn.preprocessing import StandardScaler, normalize

# ---- Output paths ----
SCALER_MODEL_PATH = "quantum_stock_scaler.pkl"
QUANTUM_MATRIX_PATH = "quantum_stock_features.pkl"

# ---- Step 1: Load datasets ----
train_df = pd.read_csv("ADANIPORTS.csv")
full_df = pd.read_csv("BSE.csv")

# ---- Step 2: Preprocess stock data ----
def preprocess_stock_data(df, calculate_indicators=True):
    """
    Extract numerical features from stock data and optionally calculate indicators.
    """
    df = df.copy()

    # Convert Date to datetime if present
    if 'Date' in df.columns:
        df['Date'] = pd.to_datetime(df['Date'])
        df = df.sort_values('Date')

    # Select basic numerical columns
    feature_columns = []
    for col in ['Open', 'High', 'Low', 'Close', 'Volume']:
        if col in df.columns:
            feature_columns.append(col)

    # Calculate technical indicators only if requested and columns exist
    if calculate_indicators and 'Close' in df.columns:
        # Daily returns (percentage change)
        df['Returns'] = df['Close'].pct_change()
        # Moving average (5-day)
        if len(df) >= 5:
            df['MA_5'] = df['Close'].rolling(window=5).mean()
        # Price momentum (10-day change)
        if len(df) >= 10:
            df['Momentum'] = df['Close'] - df['Close'].shift(10)

    if calculate_indicators and 'High' in df.columns and 'Low' in df.columns:
        # Intraday price range
        df['Price_Range'] = df['High'] - df['Low']
        # Price volatility indicator
        df['Volatility'] = ((df['High'] - df['Low']) / df['Close']) * 100

    if calculate_indicators and 'Open' in df.columns and 'Close' in df.columns:
        # Daily price change
        df['Daily_Change'] = df['Close'] - df['Open']
        # Gap indicator
        df['Gap'] = df['Open'] - df['Close'].shift(1)

    # Remove NaN values created by calculations
    df = df.dropna()

    # Identify all potential feature columns created
    all_potential_features = ['Open', 'High', 'Low', 'Close', 'Volume',
                              'Returns', 'MA_5', 'Momentum', 'Price_Range',
                              'Volatility', 'Daily_Change', 'Gap']

    # Filter to only include columns actually present in the DataFrame
    available_features = [col for col in all_potential_features if col in df.columns]

    return df, available_features

# Preprocess datasets to identify common features
train_df_processed, train_available_features = preprocess_stock_data(train_df, calculate_indicators=True)
full_df_processed, full_available_features = preprocess_stock_data(full_df, calculate_indicators=True)

# Determine common features to use for scaling
common_features = list(set(train_available_features) & set(full_available_features))
print(f"Common features available in both datasets: {common_features}")

# Ensure we have exactly 8 features for 8 qubits
if len(common_features) > 8:
    # Take the first 8 common features
    features_to_use = common_features[:8]
elif len(common_features) < 8:
    # Pad with zero columns if needed in both dataframes
    features_to_use = common_features
    padding_count = 8 - len(common_features)
    for i in range(padding_count):
        pad_col_name = f'padding_{i}'
        train_df_processed[pad_col_name] = 0.0
        full_df_processed[pad_col_name] = 0.0
        features_to_use.append(pad_col_name)
else:
    features_to_use = common_features

print(f"Using features for scaling and encoding: {features_to_use}")
print(f"Training samples: {len(train_df_processed)}, Full dataset samples: {len(full_df_processed)}")

# ---- Step 3: Feature Scaling ----
# Scale features to [0, œÄ] range for quantum rotation angles
scaler = StandardScaler()
scaler.fit(train_df_processed[features_to_use]) # Fit on the common features from training data

# Transform full dataset using the fitted scaler
scaled_features = scaler.transform(full_df_processed[features_to_use])

# Normalize to [0, œÄ] for quantum angle embedding
# Map from standardized values to rotation angles
scaled_features = np.arctan(scaled_features) + np.pi/2  # Maps to roughly [0, œÄ]
scaled_features = normalize(scaled_features, norm="l2")  # L2 normalize


# ---- Step 4: Quantum device setup ----
n_qubits = 8  # Fixed at 8 qubits for quantum encoding
dev = qml.device("default.qubit", wires=n_qubits)

# ---- Step 5: Define quantum feature encoder ----
@qml.qnode(dev)
def quantum_stock_encoder(x):
    """
    Encode stock features into quantum circuit:
    - AngleEmbedding: Encodes features as rotation angles
    - BasicEntanglerLayers: Creates entanglement between qubits
    - Measure expectation values to get quantum features
    """
    # Ensure input vector matches number of qubits
    if len(x) != n_qubits:
        raise ValueError(f"Input vector length ({len(x)}) must match number of qubits ({n_qubits})")

    # Encode classical features into qubit rotations
    qml.templates.AngleEmbedding(x, wires=range(n_qubits))

    # Create entanglement between features
    # Using trainable weights (initialized to ones for simplicity)
    weights = np.ones((2, n_qubits))  # 2 layers of entanglement
    qml.templates.BasicEntanglerLayers(weights=weights, wires=range(n_qubits))

    # Additional variational layer for richer feature extraction
    for i in range(n_qubits):
        qml.RY(0.1 * x[i % len(x)], wires=i)

    # Measure expectation values (quantum features)
    return [qml.expval(qml.PauliZ(i)) for i in range(n_qubits)]

# ---- Step 6: Encode all samples ----
print("Encoding stock data into quantum features...")
quantum_features = np.array([quantum_stock_encoder(x) for x in scaled_features])
print(f"Quantum features shape: {quantum_features.shape}")

# ---- Step 7: Save models and features ----
# Save scaler for preprocessing new data
joblib.dump(scaler, SCALER_MODEL_PATH)

# Save quantum features and metadata
joblib.dump({
    "quantum_features": quantum_features,
    "scaled_features": scaled_features,
    "feature_names": features_to_use, # Save the actual features used
    "df": full_df_processed, # Save the processed dataframe
    "n_qubits": n_qubits
}, QUANTUM_MATRIX_PATH)

print(f"\n‚úÖ Quantum stock scaler saved to '{SCALER_MODEL_PATH}'")
print(f"‚úÖ Quantum feature matrix saved to '{QUANTUM_MATRIX_PATH}'")
print(f"üìä Processed {len(full_df_processed)} stock data samples")
print(f"üî¨ Generated {quantum_features.shape[1]} quantum features per sample")

# ---- Optional: Display sample statistics ----
print("\nüìà Sample Quantum Features (first 5 rows):")
print(quantum_features[:5])
print("\nüìä Feature Statistics:")
print(f"Mean: {np.mean(quantum_features, axis=0)}")
print(f"Std: {np.std(quantum_features, axis=0)}")

import joblib
import numpy as np
import pennylane as qml
from sklearn.preprocessing import StandardScaler, normalize
from sklearn.metrics.pairwise import cosine_similarity
import pandas as pd

# --------------------------------------------------
# Load quantum stock model and quantum embeddings
# --------------------------------------------------
SCALER_MODEL_PATH = "quantum_stock_scaler.pkl"
QUANTUM_MATRIX_PATH = "quantum_stock_features.pkl"

scaler = joblib.load(SCALER_MODEL_PATH)
data = joblib.load(QUANTUM_MATRIX_PATH)

quantum_features = data["quantum_features"]
scaled_features = data["scaled_features"]
feature_names = data["feature_names"]
df = data["df"]
n_qubits = data["n_qubits"]

print(f"üìä Loaded {len(df)} stock records with {n_qubits} quantum features")
print(f"üìà Features used: {feature_names}")

# --------------------------------------------------
# Quantum device setup
# --------------------------------------------------
dev = qml.device("default.qubit", wires=n_qubits)

@qml.qnode(dev)
def quantum_stock_encoder(x):
    """Quantum feature map: encodes stock features into qubits"""
    qml.templates.AngleEmbedding(x, wires=range(n_qubits))

    # Create entanglement between features
    weights = np.ones((2, n_qubits))
    qml.templates.BasicEntanglerLayers(weights=weights, wires=range(n_qubits))

    # Additional variational layer
    for i in range(n_qubits):
        qml.RY(0.1 * x[i % len(x)], wires=i)

    return [qml.expval(qml.PauliZ(i)) for i in range(n_qubits)]

# --------------------------------------------------
# Quantum stock pattern search functions
# --------------------------------------------------
def find_similar_patterns(target_date, top_k=5):
    """
    Find historical patterns similar to a specific date's market behavior.
    Useful for pattern recognition and trend analysis.
    """
    # Find the target date in dataframe
    if 'Date' in df.columns:
        df['Date'] = pd.to_datetime(df['Date'])
        target_row = df[df['Date'] == pd.to_datetime(target_date)]
    else:
        print("‚ùå Date column not found in dataframe")
        return

    if target_row.empty:
        print(f"‚ùå Date {target_date} not found in dataset")
        return

    target_idx = target_row.index[0]
    target_quantum = quantum_features[target_idx]

    # Compute similarity with all other dates
    sims = cosine_similarity([target_quantum], quantum_features).flatten()
    top_idx = sims.argsort()[::-1][:top_k+1]  # +1 to exclude self

    # Display results (skip first as it's the query itself)
    print(f"\nüîç Finding patterns similar to: {target_date}")
    print(f"Target metrics: Close={df.iloc[target_idx]['Close']:.2f}, "
          f"High={df.iloc[target_idx]['High']:.2f}, Low={df.iloc[target_idx]['Low']:.2f}")
    print("\n" + "="*80)

    for rank, idx in enumerate(top_idx[1:], 1):  # Skip first (self-match)
        row = df.iloc[idx]
        print(f"\nüìä Rank {rank} | Similarity = {sims[idx]:.4f}")
        print(f"üìÖ Date: {row['Date']}")
        print(f"üí∞ Open: {row['Open']:.2f} | High: {row['High']:.2f} | "
              f"Low: {row['Low']:.2f} | Close: {row['Close']:.2f}")

        # Show technical indicators if available
        if 'Returns' in row and not pd.isna(row['Returns']):
            print(f"üìà Returns: {row['Returns']*100:.2f}%", end="")
        if 'Volatility' in row and not pd.isna(row['Volatility']):
            print(f" | Volatility: {row['Volatility']:.2f}%", end="")
        if 'Momentum' in row and not pd.isna(row['Momentum']):
            print(f" | Momentum: {row['Momentum']:.2f}")
        else:
            print()

def search_by_price_pattern(open_price, high_price, low_price, close_price, top_k=5):
    """
    Find historical dates with similar price patterns to given OHLC values.
    Useful for what-if analysis and prediction.
    """
    # Create feature vector from input prices
    input_features = {}
    for feature in feature_names:
        if feature == 'Open':
            input_features[feature] = open_price
        elif feature == 'High':
            input_features[feature] = high_price
        elif feature == 'Low':
            input_features[feature] = low_price
        elif feature == 'Close':
            input_features[feature] = close_price
        elif feature.startswith('padding_'):
            input_features[feature] = 0.0
        else:
            # For calculated features, use approximations
            if feature == 'Returns':
                input_features[feature] = (close_price - open_price) / open_price if open_price != 0 else 0
            elif feature == 'Price_Range':
                input_features[feature] = high_price - low_price
            elif feature == 'Volatility':
                input_features[feature] = ((high_price - low_price) / close_price * 100) if close_price != 0 else 0
            elif feature == 'Daily_Change':
                input_features[feature] = close_price - open_price
            else:
                input_features[feature] = 0.0  # Default for features we can't calculate

    # Convert to array in correct order
    input_vector = np.array([input_features[f] for f in feature_names])

    # Scale and normalize
    input_scaled = scaler.transform(input_vector.reshape(1, -1))
    input_scaled = np.arctan(input_scaled) + np.pi/2
    input_scaled = normalize(input_scaled, norm="l2")

    # Quantum encode
    query_quantum = np.array(quantum_stock_encoder(input_scaled[0]))

    # Find similar patterns
    sims = cosine_similarity([query_quantum], quantum_features).flatten()
    top_idx = sims.argsort()[::-1][:top_k]

    print(f"\nüîç Searching for patterns similar to:")
    print(f"üí∞ Open: {open_price:.2f} | High: {high_price:.2f} | Low: {low_price:.2f} | Close: {close_price:.2f}")
    print("\n" + "="*80)

    for rank, idx in enumerate(top_idx, 1):
        row = df.iloc[idx]
        print(f"\nüìä Rank {rank} | Quantum Similarity = {sims[idx]:.4f}")
        print(f"üìÖ Date: {row['Date']}")
        print(f"üí∞ Open: {row['Open']:.2f} | High: {row['High']:.2f} | "
              f"Low: {row['Low']:.2f} | Close: {row['Close']:.2f}")

        if 'Returns' in row and not pd.isna(row['Returns']):
            print(f"üìà Returns: {row['Returns']*100:.2f}%", end="")
        if 'Volatility' in row and not pd.isna(row['Volatility']):
            print(f" | Volatility: {row['Volatility']:.2f}%")
        else:
            print()

def find_high_volatility_days(top_k=10):
    """
    Find days with highest quantum feature variance (potential trading opportunities).
    """
    # Calculate variance across quantum features for each sample
    variances = np.var(quantum_features, axis=1)
    top_idx = variances.argsort()[::-1][:top_k]

    print(f"\nüî• Top {top_k} High Volatility Days (Quantum Feature Variance)")
    print("="*80)

    for rank, idx in enumerate(top_idx, 1):
        row = df.iloc[idx]
        print(f"\nüìä Rank {rank} | Quantum Variance = {variances[idx]:.4f}")
        print(f"üìÖ Date: {row['Date']}")
        print(f"üí∞ Open: {row['Open']:.2f} | High: {row['High']:.2f} | "
              f"Low: {row['Low']:.2f} | Close: {row['Close']:.2f}")

        if 'Volatility' in row and not pd.isna(row['Volatility']):
            print(f"üìà Intraday Volatility: {row['Volatility']:.2f}%")

# --------------------------------------------------
# Example queries
# --------------------------------------------------
print("\n" + "="*80)
print("üî¨ QUANTUM STOCK PATTERN SEARCH SYSTEM")
print("="*80)

# Example 1: Find similar patterns to a specific date
print("\n\n1Ô∏è‚É£ PATTERN SIMILARITY SEARCH BY DATE")
if len(df) > 0:
    example_date = df.iloc[10]['Date'] if len(df) > 10 else df.iloc[0]['Date']
    find_similar_patterns(example_date, top_k=3)

# Example 2: Search by custom price pattern
print("\n\n2Ô∏è‚É£ PATTERN SEARCH BY CUSTOM OHLC VALUES")
if len(df) > 0:
    # Use average prices from dataset as example
    avg_open = df['Open'].mean()
    avg_high = df['High'].mean()
    avg_low = df['Low'].mean()
    avg_close = df['Close'].mean()
    search_by_price_pattern(avg_open, avg_high, avg_low, avg_close, top_k=3)

# Example 3: Find high volatility days
print("\n\n3Ô∏è‚É£ HIGH VOLATILITY PATTERN DETECTION")
find_high_volatility_days(top_k=5)

print("\n\n" + "="*80)
print("‚úÖ Quantum stock pattern analysis complete!")
print("="*80)